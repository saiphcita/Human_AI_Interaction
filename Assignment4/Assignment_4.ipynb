{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Assignment 4",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5JrhvypiDFf"
      },
      "source": [
        "from fastai.text import *\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BnTDPuQiDFi"
      },
      "source": [
        "# Language models\n",
        "\n",
        "A language model is an algorithm that takes a sequence of words, and outputs the likely next word in the sequence. Most language models output a list of words, each with its probability of occurance. For example, if we had a sentence that started `I would like to eat a hot`, then ideally the algorithm would predict that  the word `dog` had a much higher chance of being the next word than the word `meeting`. \n",
        "\n",
        "Language models are a very powerful building block in natural language processing. They are used for classifying text (e.g. is this review positive or negative?), for answering questions based on text (e.g. \"what is the capital of Finland?\" based on the Wikipedia page on Finland), and language translation (e.g. English to Japanese).\n",
        "\n",
        "## The intuition behind why language models are so broadly useful\n",
        "How can this simple sounding algorithm be that broadly useful? Intuitively, this is because predicting the next word in a sentence requires a lot of information, not just about grammar and syntax, but also about semantics: what things mean in the real-world. For instance, we know that `I would like to eat a hot dog` is semantically reasonable, but `I would like to eat a hot cat` is nonsensical. \n",
        "\n",
        "We trained a simple language model, and asked it to predict the word following `I would like to eat a `. \n",
        "\n",
        "We get:\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5tNz3_LiDFj"
      },
      "source": [
        "# Step 1: Load all the data \n",
        "In this example, we are going to use a dataset of tweets from [the Onion](https://www.theonion.com), as well as some non-sarcastic news sources. We found this data set on [Kaggle](https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection). \n",
        "\n",
        "Before we started creating this notebook, we downloaded to our computer the JSON file with the dataset of tweets to our computer. You can download the  JSON dataset from our github:\n",
        "https://github.com/saiphcita/Human_AI_Interaction/tree/main/Assignment4 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "2BSVGJZmpDmV",
        "outputId": "25ad67a9-4707-4bdf-ec83-db0178e158a0"
      },
      "source": [
        "#Code to upload the JSON dataset from your computer as a \n",
        "#The data is in a JSON file, so we are using the \"read_json\" method. \n",
        "#If your data is CSV, use the `read_csv` method instead. \n",
        "#We use the `lines=True` argument here because the author formatted each line \n",
        "#as a separate JSON object. At least half of your time as a data \n",
        "#scientist/AI researcher is spent dealing with other people's data formats!\n",
        "\n",
        "from google.colab import files \n",
        "from __future__ import print_function\n",
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "uploaded = files.upload()\n",
        "nameFile=\"Sarcasm_Headlines_Dataset_v2.json\"\n",
        "bytesFile=io.BytesIO(uploaded[nameFile])\n",
        "headlines = pd.read_json(bytesFile, lines=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-51ecaf67-7001-4b9e-a55b-4c29d47e49e8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-51ecaf67-7001-4b9e-a55b-4c29d47e49e8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Sarcasm_Headlines_Dataset_v2.json to Sarcasm_Headlines_Dataset_v2 (1).json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwELNfQYiDFl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "f606a4ce-1673-41d4-fd98-e95a941a1239"
      },
      "source": [
        "headlines"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>is_sarcastic</th>\n",
              "      <th>headline</th>\n",
              "      <th>article_link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
              "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>dem rep. totally nails why congress is falling...</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>inclement weather prevents liar from getting t...</td>\n",
              "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>mother comes pretty close to using word 'strea...</td>\n",
              "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28614</th>\n",
              "      <td>1</td>\n",
              "      <td>jews to celebrate rosh hashasha or something</td>\n",
              "      <td>https://www.theonion.com/jews-to-celebrate-ros...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28615</th>\n",
              "      <td>1</td>\n",
              "      <td>internal affairs investigator disappointed con...</td>\n",
              "      <td>https://local.theonion.com/internal-affairs-in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28616</th>\n",
              "      <td>0</td>\n",
              "      <td>the most beautiful acceptance speech this week...</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/andrew-ah...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28617</th>\n",
              "      <td>1</td>\n",
              "      <td>mars probe destroyed by orbiting spielberg-gat...</td>\n",
              "      <td>https://www.theonion.com/mars-probe-destroyed-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28618</th>\n",
              "      <td>1</td>\n",
              "      <td>dad clarifies this not a food stop</td>\n",
              "      <td>https://www.theonion.com/dad-clarifies-this-no...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>28619 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       is_sarcastic  ...                                       article_link\n",
              "0                 1  ...  https://www.theonion.com/thirtysomething-scien...\n",
              "1                 0  ...  https://www.huffingtonpost.com/entry/donna-edw...\n",
              "2                 0  ...  https://www.huffingtonpost.com/entry/eat-your-...\n",
              "3                 1  ...  https://local.theonion.com/inclement-weather-p...\n",
              "4                 1  ...  https://www.theonion.com/mother-comes-pretty-c...\n",
              "...             ...  ...                                                ...\n",
              "28614             1  ...  https://www.theonion.com/jews-to-celebrate-ros...\n",
              "28615             1  ...  https://local.theonion.com/internal-affairs-in...\n",
              "28616             0  ...  https://www.huffingtonpost.com/entry/andrew-ah...\n",
              "28617             1  ...  https://www.theonion.com/mars-probe-destroyed-...\n",
              "28618             1  ...  https://www.theonion.com/dad-clarifies-this-no...\n",
              "\n",
              "[28619 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6nJOvyKiDFm"
      },
      "source": [
        "As you can see, some of this dataset is drawn from the onion, the rest is drawn from places like the Huffington Post which publish real news, not satire. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AltH-cmiDFm"
      },
      "source": [
        "## Step 1a: Examine the data set (5 points)\n",
        "\n",
        "Before we go off adventuring, let's first see what this dataset looks like. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9pqEo1_iDFn"
      },
      "source": [
        "### Q: How large is this dataset? Is it balanced? (1 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6xps0qpiDFn"
      },
      "source": [
        "# Insert code here to check size of dataset, and how many are positive (is_sarcastic = 1) and how many negative?\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPjZYqsCiDFo"
      },
      "source": [
        "### Q: How long on average is each headline? (4 points)\n",
        "Longer text = more information. We want to see what the length of the headline is in order to see how much information it may have. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuZ3D9OaiDFo"
      },
      "source": [
        "# Insert code here to find the average length of headline (in words)\n",
        "## Hint: see https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.count.html \n",
        "# the '\\s' regex looks for spaces.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5ZN421oiDFo"
      },
      "source": [
        "# Step 2: Build a language model that knows how to write news headlines\n",
        "\n",
        "This is the first step of our project that will be using a machine learning model. \n",
        "\n",
        "We are going to use the [fast.ai](https://fast.ai/) library to create this model. If you need help with understanding this section, look at the fast.ai documentation -- it is fantastic! The steps below are modified from the [online tutorial](https://docs.fast.ai/text.html#Quick-Start:-Training-an-IMDb-sentiment-model-with-ULMFiT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3Xa5umBiDFp"
      },
      "source": [
        "import fastai\n",
        "from fastai.text import * "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRUWwpLZiDFp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "843ab583-298b-4bd2-df34-e305c801de33"
      },
      "source": [
        "data_lm = (TextList.from_df(headlines, cols='headline').split_none().label_for_lm().databunch())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fastai/core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return np.array(a, dtype=dtype, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsBvRn9xiDFq"
      },
      "source": [
        "## So here is what happened above. \n",
        "\n",
        "First, we tell fastai that we want to work on a list of texts (headlines in our case), that are stored in a dataframe (that's the `TextList.from_df` part.)  Finally, we tell it where to look for the headline in the dataframe (which column to use, `cols=`). \n",
        "\n",
        "Then there are two other important parts. We'll take it from the end. A `databunch` is a fastai convenience. It keeps all your training, validation and test data together. But what kind of validation data do we need for a language model? Remember that a language model predicts the next word in an input sequence of words. So, we can't just take some of the headlines and set them aside as validation. Instead, we want to use all the sentences and validate whether we can guess the right next word some fraction of the time. So, we first say `split_none` so you use all your data. Then we say `label_for_lm` so it labels the \"next word\" as the label for each sequence of words. It's a clever method -- see the source if you're curious!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TsEf6fViDFq"
      },
      "source": [
        "data_lm.save('data_lm_export.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofE8kHyliDFr"
      },
      "source": [
        "Let's save this databunch. We'll use this saved copy later. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrQjH72liDFr"
      },
      "source": [
        "## Step 2a: Learn the model\n",
        "\n",
        "Now that we have the data, it's time to train the model.\n",
        "\n",
        "Now, we *could* learn a language model from scratch. But we're instead going to cheat. We're going to use a pretrained language model, and finetune it for our purpose. Specifically, we're going to use a model trained on the `Wikitext-103` corpus. \n",
        "\n",
        "One way to understand it is to think of our pre-trained model is as a model that can predict the next word in a Wikipedia article. We want to train it to write headlines instead. Since headlines still have to sound like English, ie. follow grammar, syntax, be generally plausible etc, being able to predict the next word in Wikipedia is super useful. It allows us to start with a model that already knows some English, and then just train it for writing headlines.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X_WSMdZiDFr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3dab7fdc-4290-436f-91fa-1e400311b17d"
      },
      "source": [
        "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://s3.amazonaws.com/fast-ai-modelzoo/wt103-fwd.tgz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o4-oZidiDFs"
      },
      "source": [
        "This `AWD_LSTM` is the pretrained Wikipedia model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiuY5lgTiDFs"
      },
      "source": [
        "Let's train it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X97e-xJdiDFs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "7d75b316-acb9-4748-ebd6-b688e9691f4d"
      },
      "source": [
        "learn.fit_one_cycle(1, 1e-2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>6.049117</td>\n",
              "      <td>#na#</td>\n",
              "      <td>11:35</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEqUEvisiDFt"
      },
      "source": [
        "Once trained, it's time to write some headlines! We give it a starting sequence `Students protest ` and see what it comes up with. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyt4Ceo6iDFt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6a93c114-3f8b-41bb-fc00-1c07e936eeea"
      },
      "source": [
        "learn.predict(\"Students protest \", n_words=5, no_unk=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Students protest  night in town hall station'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzAOkGBDiDFt"
      },
      "source": [
        "Pretty good, huh? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VGcV1INiDFu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3a982faa-ad12-46d8-ea26-4637859eda09"
      },
      "source": [
        "learn.predict('The Fed is expected to', n_words=3, no_unk=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Fed is expected to be a work'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjGG9RNhiDFu"
      },
      "source": [
        "OK, it's not perfect! Let's make it a little better. \n",
        "\n",
        "The `unfreeze` below is telling fastai to allow us to change the weights throughout the model. We do this when we want to make the model generate text that's more similar to our headlines (than to Wikipedia). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk6Shqj7iDFu"
      },
      "source": [
        "learn.unfreeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZsUA5agiDFu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "9e20e6f8-6950-423e-ff26-12d70bb02abe"
      },
      "source": [
        "learn.fit_one_cycle(cyc_len=1, max_lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>5.169644</td>\n",
              "      <td>#na#</td>\n",
              "      <td>17:31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWeFTZXkiDFv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "921d78f6-02b7-48dc-ebf0-ad9b6bf92223"
      },
      "source": [
        "learn.predict('New Study', n_words=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'New Study finds 65 % of our'"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnvtEh60iDFv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5fd21e3e-3725-455e-b73e-b8107341f617"
      },
      "source": [
        "learn.predict('16 Problems', n_words=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'16 Problems of the past , 15'"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy5-xdC2iDFw"
      },
      "source": [
        "OK, now let's save our hard work. We'll use this later. (Pssst: why is it called an encoder? Look at the Fastai docs to find out!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QARD_fICiDFw"
      },
      "source": [
        "learn.save_encoder('headlines-awd.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvl9Phf8iDFw"
      },
      "source": [
        "Note that we also want to save the whole model, so we can reuse it in our twitter bot. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MVHxRcviDFw"
      },
      "source": [
        "learn.export('headlines-lm.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UI9AVOiiDFw"
      },
      "source": [
        "# Step 2b: See how well the language model works (15 points)\n",
        "\n",
        "Try generating a few more headlines. Then, answer the following questions. Wherever possible, show what code you ran, or what predictions you asked it for. *Suggestion: Try using punctuations, numbers, texts of different lengths etc.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYxXBKgviDFx"
      },
      "source": [
        "### Q: What is the effect of starting with longer strings? (5 points)\n",
        "\n",
        "We could start our headline generation with just one word, e.g. `learn.predict('White', n_words=9)` or with many: `learn.predict('White House Says Whistleblower Did', n_words=5)`. \n",
        "\n",
        "State in your Notebook: What is the difference you see in the kinds of headlines generated?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpmji1DBiDFx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4bc7c50f-0691-43a5-edad-72277fc41852"
      },
      "source": [
        "## Your answer here. Insert more cells if you want to insert code etc.\n",
        "learn.predict('White', n_words=9)\n",
        "#learn.predict('White House Says Whistleblower Did', n_words=5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'White mayor of transgender candidate just choosing to have a'"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uamII3UiDFx"
      },
      "source": [
        "## Q: What aspects of the task of generating headlines does our language model do well? (5 points)\n",
        "For example, does it get grammar right? Does it know genders of people or objects? etc. State the response in your Notebook. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArtWUq2BiDFy"
      },
      "source": [
        "#Your answer here. Insert more cells if you want to insert code etc.\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCxEO_g0iDFy"
      },
      "source": [
        "## Q: What aspects of the task of generating headlines does our model do poorly? (5 points)\n",
        "What does it frequently get wrong? Why might it make these mistakes?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1VeDdTJiDFy"
      },
      "source": [
        "## Your answer here\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U11W17BiDFy"
      },
      "source": [
        "# Step 3: Learn a classifier to see which headlines are satire\n",
        "\n",
        "Remember, our dataset has some stories that are satire (from the Onion) and others that are real. Now, we're going to train a classifier to distinguish one from the other. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqhhUQjYiDFy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "17df891c-990f-46af-d393-c78ab945eb6f"
      },
      "source": [
        "data_clas = (TextList.from_df(df=headlines, vocab= data_lm.train_ds.vocab, cols='headline').split_by_rand_pct(valid_pct=0.2).label_from_df(cols='is_sarcastic').databunch())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fastai/core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return np.array(a, dtype=dtype, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "567waxMAiDFz"
      },
      "source": [
        "We're using a similar databunch method as we did for our language model above. Here, we are using `split_by_rand_pct` so we keep some fraction of our dataset as a validation set. There is one other trick: `vocab= data_lm.train_ds.vocab` ensures that our classifier only uses words that we have in our language model -- so it never deals with words it hasn't encountered before. (Consider: why is this important?)\n",
        "\n",
        "See if you can work out what the other arguments are. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTK9yKJPiDFz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "ee7eb551-4e5d-4b7e-c4ed-d7112e566718"
      },
      "source": [
        "data_clas.show_batch()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>xxbos ' 12 years a slave , ' ' captain xxunk , ' ' american xxunk , ' ' wolf of wall street , ' ' blue xxunk , ' ' dallas buyers club , ' ' her , ' ' xxunk , ' ' before midnight , ' and ' xxunk ' all written during same continuing education screenwriting class</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos past xxunk and on to xxunk , one of israel 's premier xxunk sites : spring break 2016 , breaking bad on the looney front - part 1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos trump : ' the only way to find out what happened at the saudi consulate is to send in more journalists one at a time '</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos ' let 's all say what we 're grateful for , ' says mother who apparently believes she 's in a norman fucking xxunk painting</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos trump : ' i know that was pretty bad , but let 's just say you 're going to want to save your energy '</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0R01vVZiDF0"
      },
      "source": [
        "Above: what our data looks like after we apply the vocabulary restriction. `xxunk` is an unknown word. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCBYXSjFiDF0"
      },
      "source": [
        "Below: we're creating a classifier. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4nb7SMxiDF0"
      },
      "source": [
        "classify = text_classifier_learner(data=data_clas, arch=AWD_LSTM, drop_mult=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9W3dOdtiDF0"
      },
      "source": [
        "Remember that language model we saved earlier? It's time load it back!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvD852qHiDF0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31bbaf0a-652c-441e-93e2-a886e528c3bb"
      },
      "source": [
        "classify.load_encoder('headlines-awd.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (22896 items)\n",
              "x: TextList\n",
              "xxbos xxunk scientists unveil doomsday clock of hair loss,xxbos dem rep . totally nails why congress is falling short on gender , racial equality,xxbos eat your xxunk : 9 xxunk different recipes,xxbos xxunk weather prevents liar from getting to work,xxbos mother comes pretty close to using word ' streaming ' correctly\n",
              "y: CategoryList\n",
              "1,0,0,1,1\n",
              "Path: .;\n",
              "\n",
              "Valid: LabelList (5723 items)\n",
              "x: TextList\n",
              "xxbos larry xxunk ' leaning ' toward senate run in connecticut,xxbos everyone doing it , xxunk sources xxunk,xxbos bloated obama delivers press conference from couch behind podium,xxbos clinton announces transition leadership should she win in november,xxbos cnn launches ' cnn for the shuttle bus from the airport to the hotel ' news channel\n",
              "y: CategoryList\n",
              "0,1,1,0,1\n",
              "Path: .;\n",
              "\n",
              "Test: None, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(11152, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(11152, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1152, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1152, 1152, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1152, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7fd8543997a0>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
              "learn: ...\n",
              "alpha: 2.0\n",
              "beta: 1.0], layer_groups=[Sequential(\n",
              "  (0): Embedding(11152, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(11152, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True, silent=False)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NatjVx_NiDF1"
      },
      "source": [
        "What's happening here? \n",
        "\n",
        "Here's the trick: a language model predicts the next word in a sequence using all the information it has so far (all the previous words). When we train a classifier, we ask it to predict the label (satire or not) instead of the next word. \n",
        "\n",
        "The intuition here is that if you can tell what the next word in a sentence is, you can tell if it is satirical. (Similarly, if you can can tell what the next word in an email is, you can tell if it is spam, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_ydDEyGiDF1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "693dff57-8fe9-4cc9-915d-9c6ac14f6933"
      },
      "source": [
        "classify.fit_one_cycle(1, 1e-2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.446148</td>\n",
              "      <td>0.368206</td>\n",
              "      <td>0.840468</td>\n",
              "      <td>05:37</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwJ09DkpiDF1"
      },
      "source": [
        "classify.freeze_to(-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pz1tevLiDF2"
      },
      "source": [
        "Above: this is similar to `unfreeze()` that we used before. Except, you only allow a few layers of your model to change. Then we can train again, similar to using `unfreeze()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nt_EGGrLiDF2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "f953bcc6-560d-414e-8cd0-2d922b8eec5c"
      },
      "source": [
        "classify.fit_one_cycle(1, 1e-3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.385368</td>\n",
              "      <td>0.320701</td>\n",
              "      <td>0.865280</td>\n",
              "      <td>06:17</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k-pncidiDF2"
      },
      "source": [
        "Wow! An accuracy of 86%! That sounds great, and for not that much work. \n",
        "\n",
        "Now, let's try it on some headlines, to see how well it does. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOYf2qIAiDF2"
      },
      "source": [
        "# Step 4: try out the classifier (20 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP1XqMiBiDF2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a580fd2-8227-422d-ea83-ecfe72259105"
      },
      "source": [
        "classify.predict(\"Despair for Many and Silver Linings for Some in California Wildfires\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Category tensor(0), tensor(0), tensor([0.9891, 0.0109]))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smpud4qliDF3"
      },
      "source": [
        "Here in the output, the first part of this tuple is the chosen category (`0`, i.e. not satire), and the last part is an array of probabilities. The classifier suggests that the headline (which I got from the [New York Times](https://www.nytimes.com/2019/10/29/us/california-fires-homes.html?action=click&module=Top%20Stories&pgtype=Homepage)) is not satire, with about a 98% confidence. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDTIC0vriDF3"
      },
      "source": [
        "## Step 4a: Try out this classifier (10 points)\n",
        "\n",
        "Below, try the classifier with some headlines, real or made up (including made up by the language model above). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAIh0AXOiDF3"
      },
      "source": [
        "## Two headlines that the classifier correctly classifies (1 point)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRn0iQj2iDF3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vf8Dx3yiDF4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCjxcXvMiDF4"
      },
      "source": [
        "## Two headlines that the classifier classifies incorrectly (1 point)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pavbT8sbiDF4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5zPcXLfiDF4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xvkl2ba1iDF4"
      },
      "source": [
        "Now, we want to find two headlines that the classifier is really confident about, but classifies incorrectly. We want the confidence of the prediction to be at least 85%.\n",
        "\n",
        "One headline is anything you want to write. Another must be a real headline (not satire) that you could trick the classifier into misclassifying changing only one word. For instance, taking `\"Despair for Many and Silver Linings for Some in California Wildfires\"`, a real NYTimes headline, you can change it to `\"Despair for Many and Silver Linings for Some in Oregon Wildfires\"` (note that this particular change does not cause the classifier to misclassify)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QFTzngWiDF4"
      },
      "source": [
        "## Insert one headline that the classifier classifies incorrectly, with false high confidence. (4 points)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89v4tsaRiDF5"
      },
      "source": [
        "## Insert one headline that the classifier classifies incorrectly, with false high confidence. (4 points)\n",
        "\n",
        "# Also, insert link to the original headline/article.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD4NA76ViDF5"
      },
      "source": [
        "## Step 4b: What kinds of headlines are misclassified? (10 points)\n",
        "\n",
        "Write your hypothesis below on what kinds of headlines are misclassified. If it helps you, use the [TextClassificationInterpretation](https://docs.fast.ai/text.learner.html#TextClassificationInterpretation) utility. Show your work, especially if you use this utility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0VS_J40iDF5"
      },
      "source": [
        "## Show work here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8J7iQgiiDF5"
      },
      "source": [
        "(Add your interpretation here)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUnxRLm0iDF6"
      },
      "source": [
        "# Step 5: Save your classifier\n",
        "Now that we've trained the classifier, you're ready for Part 2. You'll use this saved file in your bot later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GK_Vd1OhiDF6"
      },
      "source": [
        "classify.export(file='satire_awd.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0X44e5hiDF6"
      },
      "source": [
        "Later, you'll use it like so."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ra53eLBfYbz",
        "outputId": "692d5577-9817-4de0-9e0c-35d3149c10f1"
      },
      "source": [
        "\n",
        "#You will want to download the classifiers to your google drive to use them for \n",
        "#this part. Download pre-trained learners here or use your own:\n",
        "#https://drive.google.com/drive/folders/1KrYsysOojlOvVK-YfSehREsiz1k0Xxme?usp=sharing\n",
        "#use this tutorial to understand how to access and store some of this data\n",
        "#on your google drive for easy access:  \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "piLM7W_QjvKS",
        "outputId": "48525c26-b9c2-4237-a15a-6fe92849e82e"
      },
      "source": [
        "data_path=\"/content/\"\n",
        "serve_classifier = load_learner(path=data_path, file='satire_awd.pkl')\n",
        "serve_lm = load_learner(path=data_path, file='headlines-lm.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4E0R3MhMj9pj",
        "outputId": "39acd3e5-53b2-4aa5-b584-435253d849fb"
      },
      "source": [
        "#this will use the learner you uploaded to predict whether a certain sentence \n",
        "#is about satire or not. \n",
        "serve_classifier.predict('How the New Syria Took Shape')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Category tensor(0), tensor(0), tensor([0.9880, 0.0120]))"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SNCypmfGkMEQ",
        "outputId": "47e17b7a-0b09-4ac2-c01e-6ed33dc8fdfd"
      },
      "source": [
        "#this will create a headline based on a phrase you provide\n",
        "#and based on what your learner learned\n",
        "serve_lm.predict('Rising Seas', n_words=7)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Rising Seas : a awful love cook , few'"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzTDLTBBiDF7"
      },
      "source": [
        "# Step 6: add the bot code. \n",
        "\n",
        "See the assignment document for what the bot code should look like. You can add it just below here, but you are also welcome to create a new notebook where you put that code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPFUcloPiDF7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}